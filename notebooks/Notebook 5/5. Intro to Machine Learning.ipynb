{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780d4354-2f54-4809-866a-78143878b4d8",
   "metadata": {},
   "source": [
    "# Notebook #5: Intro to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f57b07-69a0-426c-bfbf-3c9e84e92e54",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "###  1) What is Machine Learning?\n",
    "###  2) Basic ideas of Machine Learning in Python\n",
    "###  3) Logistic Regression\n",
    "###  4) Random Forest\n",
    "###  5) Support Vector Machines\n",
    "###  6) Neural Networks\n",
    "###  7) Example with GeoChem Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5cb64-8530-4a93-9577-6ada12605a38",
   "metadata": {},
   "source": [
    "###  1) What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549d1b7-35bb-4a19-95c9-7a3da3c9effb",
   "metadata": {},
   "source": [
    "#### Machine learning is a subset of AI which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy to solve growingly complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75304e4-a8a4-4def-af02-41fb712c095c",
   "metadata": {},
   "source": [
    "#### Machine learning is an important component of the growing field of data science. The models we make with ML are trained to make classifications or predictions, and to uncover key insights in data mining projects. These insights subsequently drive decision making within applications and businesses, ideally impacting key growth metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f50bcd-7e98-4193-90cb-9c70fa96b2ec",
   "metadata": {},
   "source": [
    "###  2) Basic ideas of Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c1818-af7f-4c79-b649-18766a459472",
   "metadata": {},
   "source": [
    "#### There's 4 main kinds of machine learning: supervised learning, semi-supervised learning, unsupervised learning, and reinforcement learning.\n",
    "\n",
    "#### Supervised learning examples:\n",
    "#### -Classification\n",
    "#### -Regression\n",
    "\n",
    "#### Unsupervised learning examples:\n",
    "#### -Clustering\n",
    "#### -Association\n",
    "#### -Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63e0e4-c327-4270-a2b8-264129876525",
   "metadata": {},
   "source": [
    "#### Some of the Python tools we'll use are\n",
    "\n",
    "#### - NumPy\n",
    "#### - SciPy\n",
    "#### - Scikit-learn\n",
    "#### - TensorFlow\n",
    "#### - And many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac71a3c-2f27-493c-b38a-c43a9c8e4bc0",
   "metadata": {},
   "source": [
    "###  3) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b2961-683c-4ac8-b59a-642ef3bfcdaa",
   "metadata": {},
   "source": [
    "#### Logistic regression classification algorithm used to predict the probability of a variable. This essentially boils down to a binary classification. Its is also possibly to have multinomial and ordinal regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061b9d7-0bbf-4119-be6f-6c392f6f78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets see how we can do one in python using an example dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits() # Load the example dataset\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "plt.figure(figsize=(6, 6)) # Plot the example dataset for initial visual\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='g', label='0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='y', label='1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88c5a8-ce28-4b0c-8bcc-5e38b9d0e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are spliting up our data into the test and train split with the functino on the right that does it randomly \n",
    "## according to the test_size you assign.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af213f-5cf7-43d4-b260-40408790f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, we create a model object\n",
    "digreg = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a026327-9b53-4d73-967f-2384b9c9e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then, we fit othe model object to our data\n",
    "digreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe11acb-8841-40bc-a22e-1cc5152d3620",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, by using the test Xs, we can see how well our trained model can create results\n",
    "y_pred = digreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672b7c2-f81a-4452-8d42-5df6d2179270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Logistic Regression model is:\",\n",
    "metrics.accuracy_score(y_test, y_pred)*100) # With this line we can show off how accurate our model is at its predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faedf23b-61fd-4dc9-b695-4003b2923557",
   "metadata": {},
   "source": [
    "###  4) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012c49c-f84f-4f66-a29d-05fb99ed4c7b",
   "metadata": {},
   "source": [
    "#### Random forest is a supervised learning algorithm which is used for both classification as well as regression. A forest is made up of decision trees and more decision trees means more robust forest. Similarly, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ddda8-7dcc-45e6-ac1b-709d4f38c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658824bc-418d-4edd-84c7-96802ac7ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split up our data into the test and train split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923578f-33a5-4ed9-ba42-9ae7c241cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, we create a model object\n",
    "classifier = RandomForestClassifier(n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee0bc2-13de-41a3-9242-9d641d442bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then, we fit othe model object to our data\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadfaeb-fa81-4d80-9975-d3ba5e38b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, by using the test Xs, we can see how well our trained model can create results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192734d7-b08f-4a86-8504-3fabdb79525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1)\n",
    "result2 = accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy:\",result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934deaac-55c8-4e8c-a6d5-69d38fe387c0",
   "metadata": {},
   "source": [
    "#### With this second example, we can see that the pipeline for using premade models like this are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f2bf1-f80c-4a1e-851a-725ff436a12c",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "sns.set()###  5) Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4a917-598c-4b5f-b999-c4010b6540b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523721a6-cb56-4b63-9da0-cbab7dfee9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the max and min values for the boundaries on the plot\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "h = (x_max / x_min)/100\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) \n",
    "\n",
    "X_plot = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "C = 1.0 # Value of regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd943d-ceb8-47cd-9c1a-93c4e0fd2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = svm.SVC(kernel='linear', C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e80a0-9cf1-4670-b02c-2930c99b1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971043f-f68d-42a5-a85a-783c7c6c802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = svc_classifier.predict(X_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03d0d6-13a3-47eb-9e7b-77e9347fe7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24985152-a3f0-4e31-9068-5bbb4e000ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.title('Support Vector Classifier with linear kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081008b5-c43b-448d-950a-1110085d6aaa",
   "metadata": {},
   "source": [
    "###  6) Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2617382-739f-4e47-a227-5ab72867011c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa09c81d-fa37-417e-b25d-56353de237a9",
   "metadata": {},
   "source": [
    "### 7) Example with GeoChem Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ebbce-fd88-405a-b6ce-1b2346d3a1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
